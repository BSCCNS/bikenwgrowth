{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Analysis of bicycle network results\n",
    "## Project: Algorithmic bicycle network design\n",
    "#### Michael Szell, Tyler Perlman, Sayat Mimar, Gourab Ghoshal, Roberta Sinatra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the existing infrastructure, the results from 03_poi_based_generation and 04_connect_clusters and calculates/analyzes a number of measures:\n",
    "* cost (length)\n",
    "* coverage  \n",
    "* directness  \n",
    "* efficiency\n",
    "* overlap with existing networks\n",
    "\n",
    "Contact: Michael Szell (michael.szell@gmail.com)  \n",
    "Created: 2020-07-08  \n",
    "Last modified: 2020-11-24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False # If True, will produce plots and/or verbose output to double-check\n",
    "%run -i \"../parameters/parameters.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i path.py\n",
    "%run -i setup.py\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze existing infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for placeid, placeinfo in cities.items():\n",
    "    print(placeid + \": Analyzing existing infrastructure.\")\n",
    "    \n",
    "    # output_place is one static file for the existing city. This can be compared to the generated infrastructure.\n",
    "    # Make a check if this file was already generated - it only needs to be done once. If not, generate it:\n",
    "    filename = placeid + \"_existing.csv\"\n",
    "    if not os.path.isfile(PATH[\"results\"] + placeid + \"/\" + filename):\n",
    "        empty_metrics = {\n",
    "                         \"length\":0,\n",
    "                         \"length_lcc\":0,\n",
    "                         \"coverage\": 0,\n",
    "                         \"directness\": 0,\n",
    "                         \"directness_lcc\": 0,\n",
    "                         \"poi_coverage\": 0,\n",
    "                         \"components\": 0,\n",
    "                         \"efficiency_global\": 0,\n",
    "                         \"efficiency_local\": 0\n",
    "                        }\n",
    "        output_place = {}\n",
    "        for networktype in networktypes:\n",
    "            output_place[networktype] = copy.deepcopy(empty_metrics)\n",
    "\n",
    "        # Analyze all networks     \n",
    "        Gs = {}\n",
    "        for networktype in networktypes:\n",
    "            Gs[networktype] = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, networktype)\n",
    "            Gs[networktype + \"_simplified\"] = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, networktype + \"_simplified\")\n",
    "        \n",
    "        with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_railwaystation_nnidscarall.csv') as f:\n",
    "            nnids = [int(line.rstrip()) for line in f]\n",
    "\n",
    "        covs = {}\n",
    "        for networktype in networktypes:\n",
    "            if debug: print(placeid + \": Analyzing results: \" + networktype)\n",
    "            metrics, cov, _ = calculate_metrics(Gs[networktype], Gs[networktype + \"_simplified\"], Gs['carall'], nnids, empty_metrics, buffer_walk, buffer_overlap, numnodepairs, debug)\n",
    "            for key, val in metrics.items():\n",
    "                output_place[networktype][key] = val\n",
    "            covs[networktype] = cov\n",
    "        # Save the covers\n",
    "        write_result(covs, \"pickle\", placeid, \"\", \"\", \"existing_covers.pickle\")\n",
    "        \n",
    "        # Write to CSV\n",
    "        write_result(output_place, \"dictnested\", placeid, \"\", \"\", \"existing.csv\", empty_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze POI based results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for placeid, placeinfo in cities.items():\n",
    "    print(placeid + \": Analyzing results\")\n",
    "\n",
    "    # Load networks\n",
    "    G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "    Gexisting = {}\n",
    "    for networktype in [\"biketrack\", \"bikeable\"]:\n",
    "        Gexisting[networktype] = ig_to_shapely(csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, networktype + \"_simplified\"))\n",
    "        \n",
    "    \n",
    "    # Load POIs\n",
    "    with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + poi_source + '_nnidscarall.csv') as f:\n",
    "        nnids = [int(line.rstrip()) for line in f]\n",
    "            \n",
    "    # Load results\n",
    "    filename = placeid + '_poi_' + poi_source + \"_\" + prune_measure\n",
    "    resultfile = open(PATH[\"results\"] + placeid + \"/\" + filename + \".pickle\",'rb')\n",
    "    res = pickle.load(resultfile)\n",
    "    resultfile.close()\n",
    "    if debug: pp.pprint(res)\n",
    "         \n",
    "    # Calculate\n",
    "    # output contains lists for all the prune_quantile values of the corresponding results\n",
    "    output, covs, output_carminusbike, covs_carminusbike, output_carconstrictedbike = calculate_metrics_additively(res[\"GTs\"], res[\"GT_abstracts\"], res[\"prune_quantiles\"], G_carall, nnids, buffer_walk, buffer_overlap, numnodepairs, debug, True, Gexisting)\n",
    "    output_MST, cov_MST, _ = calculate_metrics(res[\"MST\"], res[\"MST_abstract\"], G_carall, nnids, output, buffer_walk, buffer_overlap, numnodepairs, debug, True, ig.Graph(), Polygon(), Polygon(), False, Gexisting)\n",
    "        \n",
    "    # Save the covers\n",
    "    write_result(covs, \"pickle\", placeid, poi_source, prune_measure, \"_covers.pickle\")\n",
    "    write_result(covs_carminusbike, \"pickle\", placeid, poi_source, prune_measure, \"_covers_carminusbike.pickle\")\n",
    "    write_result(cov_MST, \"pickle\", placeid, poi_source, prune_measure, \"_cover_mst.pickle\")\n",
    "        \n",
    "    # Write to CSV\n",
    "    write_result(output, \"dict\", placeid, poi_source, prune_measure, \".csv\")\n",
    "    write_result(output_carminusbike, \"dict\", placeid, poi_source, prune_measure, \"_carminusbike.csv\")\n",
    "    write_result(output_carconstrictedbike, \"dict\", placeid, poi_source, prune_measure, \"_carconstrictedbike.csv\")\n",
    "    write_result(output_MST, \"dict\", placeid, poi_source, prune_measure, \"_mst.csv\")\n",
    "    \n",
    "#     filename = placeid + '_poi_' + poi_source + \"_mst.csv\"\n",
    "#     with open(PATH[\"results\"] + placeid + \"/\" + filename, 'w') as f:\n",
    "#         w = csv.writer(f)\n",
    "#         w.writerow(output_MST.keys())\n",
    "#         w.writerow(output_MST.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: Move this cell to 06 and make a nice cover plot\n",
    "if debug:\n",
    "#     pp.pprint(output)\n",
    "    for prune_quantile, cov in covs.items():\n",
    "        fig = plt.figure(figsize=(4, 3)) # create figure object with a (width,height)\n",
    "        axes = fig.add_axes([0, 0, 1, 1]) # left, bottom, width, height (range 0 to 1)\n",
    "        if isinstance(cov, shapely.geometry.multipolygon.MultiPolygon):\n",
    "            for pol in cov: # cov is generally a MultiPolygon, so we iterate through its Polygons\n",
    "                y, x = pol.exterior.coords.xy\n",
    "                patch1 = matplotlib.patches.Polygon(np.stack((np.asarray(y), -1*np.asarray(x)), axis=-1))\n",
    "                axes.add_patch(patch1)\n",
    "        elif isinstance(cov, shapely.geometry.polygon.Polygon) and not cov.is_empty:\n",
    "            y, x = cov.exterior.coords.xy\n",
    "            patch1 = matplotlib.patches.Polygon(np.stack((np.asarray(y), -1*np.asarray(x)), axis=-1))\n",
    "            axes.add_patch(patch1)\n",
    "        axes.set_title(str(prune_quantile))\n",
    "        axes.set_ylim([51.355, 51.40])\n",
    "        axes.set_xlim([-2.40, -2.312])\n",
    "        axes.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze cluster based results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "for placeid, placeinfo in cities.items():\n",
    "    print(placeid + \": Analyzing results\")\n",
    "\n",
    "    # Load networks\n",
    "    G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "    \n",
    "    # Load POIs\n",
    "    with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + poi_source + '_nnidscarall.csv') as f:\n",
    "        nnids = [int(line.rstrip()) for line in f]\n",
    "    \n",
    "    # Load results\n",
    "    filename = placeid + '_clusters_' + prune_measure + \"_cutoff\" + cutofftype + \"{:.2f}\".format(cutoff) + \".pickle\"\n",
    "    resultfile = open(PATH[\"results\"] + placeid + \"/\" + filename,'rb')\n",
    "    res = pickle.load(resultfile)\n",
    "    resultfile.close()\n",
    "    if debug: pp.pprint(res)\n",
    "        \n",
    "        \n",
    "    # output contains lists for all the prune_quantile values of the corresponding results\n",
    "    output = {\"length\":[],\n",
    "              \"length_lcc\":[],\n",
    "              \"coverage\": [],\n",
    "              \"directness\": [],\n",
    "              \"directness_lcc\": [],\n",
    "              \"poi_coverage\": [],\n",
    "              \"components\": [],\n",
    "              \"efficiency_global\": [],\n",
    "              \"efficiency_local\": []\n",
    "             }\n",
    "\n",
    "    covs = {}\n",
    "    for GT, GT_abstract, prune_quantile in zip(res[\"GTs\"], res[\"GT_abstracts\"], res[\"prune_quantiles\"]):\n",
    "        if debug: print(prune_quantile, len(GT.vs))\n",
    "        metrics, cov = calculate_metrics(GT, GT_abstract, G_carall, nnids, output, buffer_walk, numnodepairs)\n",
    "        for key in output.keys():\n",
    "            output[key].append(metrics[key])\n",
    "        covs[prune_quantile] = cov\n",
    "    # Save the covers\n",
    "    filename = placeid + '_clusters_' + prune_measure + \"_cutoff\" + cutofftype + \"{:.2f}\".format(cutoff) + \"_covers.pickle\"\n",
    "    with open(PATH[\"results\"] + placeid + \"/\" + filename, 'wb') as f:\n",
    "        pickle.dump(covs, f)\n",
    "        \n",
    "    # Write to CSV\n",
    "    filename = placeid + '_clusters_' + prune_measure + \"_cutoff\" + cutofftype + \"{:.2f}\".format(cutoff) + \".csv\"\n",
    "    with open(PATH[\"results\"] + placeid + \"/\" + filename, 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(output.keys())\n",
    "        w.writerows(zip(*output.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(sound_file, autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OSMNX",
   "language": "python",
   "name": "osmnx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
