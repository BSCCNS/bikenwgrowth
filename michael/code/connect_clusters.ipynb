{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting existing clusters of a bicycle network\n",
    "## Project: Bicycle network analysis with Gourab, Sayat, Tyler, Michael, Roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes a city's existing bicycle network, then connects the biggest clusters following greedy triangulation. Code adapted from Tyler.\n",
    "\n",
    "Contact: Michael Szell (michael.szell@gmail.com)  \n",
    "Created: 2020-06-29  \n",
    "Last modified: 2020-07-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import osmnx as ox\n",
    "import copy\n",
    "import math\n",
    "from haversine import haversine\n",
    "import pprint\n",
    "import sys\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "\n",
    "import watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igraph    0.8.2\n",
      "osmnx     0.13.0\n",
      "watermark 2.0.2\n",
      "numpy     1.18.4\n",
      "pandas    1.0.3\n",
      "Mon Jul 06 2020 \n",
      "\n",
      "CPython 3.8.2\n",
      "IPython 7.14.0\n",
      "\n",
      "compiler   : Clang 9.0.1 \n",
      "system     : Darwin\n",
      "release    : 19.5.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 12\n",
      "interpreter: 64bit\n",
      "Git hash   :\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeid = \"paris\"\n",
    "\n",
    "cutoff = 0.8 # How many clusters should be considered (covering cutoff fraction of total length)\n",
    "BWqs = [x/20 for x in list(range(1, 21))] # Betweenness quantiles\n",
    "# BWqs = [1]\n",
    "\n",
    "datapath = \"../data/\"\n",
    "outpath = \"../output/\"\n",
    "ymirrored = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph plotting preparation functions\n",
    "def my_plot_reset(G, nids = False):\n",
    "    reset_plot_attributes(G)\n",
    "    color_nodes(G, \"red\", nids)\n",
    "    size_nodes(G, 8, nids)\n",
    "\n",
    "def reset_plot_attributes(G):\n",
    "    \"\"\"Resets node attributes for plotting.\n",
    "    All black and size 0.\n",
    "    \"\"\"\n",
    "    G.vs[\"color\"] = \"black\"\n",
    "    G.vs[\"size\"] = 0\n",
    "        \n",
    "def color_nodes(G, color = \"blue\", nids = False, use_id = True):\n",
    "    \"\"\"Sets the color attribute of a set of nodes nids.\n",
    "    \"\"\"\n",
    "    if nids is False:\n",
    "        nids = [v.index for v in G.vs]\n",
    "        use_id = False\n",
    "    if use_id:\n",
    "        for nid in set(nids):\n",
    "            G.vs.find(id = nid)[\"color\"] = color\n",
    "    else:\n",
    "        G.vs[nids][\"color\"] = color\n",
    "\n",
    "def size_nodes(G, size = 5, nids = False, use_id = True):\n",
    "    \"\"\"Sets the size attribute of a set of nodes nids.\n",
    "    \"\"\"\n",
    "    if nids is False:\n",
    "        nids = [v.index for v in G.vs]\n",
    "        use_id = False\n",
    "    if use_id:\n",
    "        for nid in set(nids):\n",
    "            G.vs.find(id = nid)[\"size\"] = size\n",
    "    else:\n",
    "        G.vs[nids][\"size\"] = size\n",
    "\n",
    "def color_edges(G, color = \"blue\", eids = False):\n",
    "    \"\"\"Sets the color attribute of a set of edge eids.\n",
    "    \"\"\"\n",
    "    if eids is False:\n",
    "        G.es[\"color\"] = color\n",
    "    else:\n",
    "        G.es[eids][\"color\"] = color\n",
    "        \n",
    "def width_edges(G, width = 1, eids = False):\n",
    "    \"\"\"Sets the width attribute of a set of edge eids.\n",
    "    \"\"\"\n",
    "    if eids is False:\n",
    "        G.es[\"width\"] = width\n",
    "    else:\n",
    "        G.es[eids][\"width\"] = width\n",
    "        \n",
    "# Other functions   \n",
    "def round_coordinates(G, r = 7):\n",
    "    for v in G.vs:\n",
    "        G.vs[v.index][\"x\"] = round(G.vs[v.index][\"x\"], r)\n",
    "        G.vs[v.index][\"y\"] = round(G.vs[v.index][\"y\"], r)\n",
    "\n",
    "def mirror_y(G):\n",
    "    for v in G.vs:\n",
    "        y = G.vs[v.index][\"y\"]\n",
    "        G.vs[v.index][\"y\"] = -y\n",
    "    \n",
    "def dist(v1,v2):\n",
    "    dist = haversine((v1['x'],v1['y']),(v2['x'],v2['y']))\n",
    "    return dist\n",
    "\n",
    "\n",
    "def osm_to_ig(node, edge):\n",
    "    \"\"\" Turns a node and edge dataframe into an igraph Graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    G = ig.Graph(directed = False)\n",
    "\n",
    "    x_coords = node['x'].tolist() \n",
    "    y_coords = node['y'].tolist()\n",
    "    ids = node['osmid'].tolist()\n",
    "    coords = []\n",
    "\n",
    "    for i in range(len(x_coords)):\n",
    "        G.add_vertex(x = x_coords[i], y = y_coords[i], id = ids[i])\n",
    "        coords.append((x_coords[i], y_coords[i]))\n",
    "\n",
    "    id_dict = dict(zip(G.vs['id'], np.arange(0, G.vcount()).tolist()))\n",
    "    coords_dict = dict(zip(np.arange(0, G.vcount()).tolist(), coords))\n",
    "\n",
    "    edge_list = []\n",
    "    for i in range(len(edge)):\n",
    "        if placeid == \"testcity\":\n",
    "            print(edge)\n",
    "        edge_list.append([id_dict.get(edge['u'][i]), id_dict.get(edge['v'][i])])\n",
    "        \n",
    "    G.add_edges(edge_list)\n",
    "    G.simplify()\n",
    "    new_edges = G.get_edgelist()\n",
    "    \n",
    "    distances_list = []\n",
    "    for i in range(len(new_edges)):\n",
    "        distances_list.append(haversine(coords_dict.get(new_edges[i][0]), coords_dict.get(new_edges[i][1])))\n",
    "\n",
    "    G.es()['weight'] = distances_list\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/misz/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3062: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "node_biketrack = pd.read_csv(datapath+placeid+'_biketrack_nodes.csv')\n",
    "edge_biketrack = pd.read_csv(datapath+placeid+'_biketrack_edges.csv')\n",
    "G_biketrack = osm_to_ig(node_biketrack, edge_biketrack)\n",
    "\n",
    "node_carall = pd.read_csv(datapath+placeid+'_carall_nodes.csv')\n",
    "edge_carall = pd.read_csv(datapath+placeid+'_carall_edges.csv')\n",
    "G_carall = osm_to_ig(node_carall, edge_carall)\n",
    "\n",
    "# Merging biketrack and carall for routing between biketrack clusters\n",
    "node_biketrackcarall = pd.concat([node_biketrack, node_carall], ignore_index = True).drop_duplicates(subset = \"osmid\", ignore_index = True, keep = \"first\").reset_index()\n",
    "edge_biketrackcarall = pd.concat([edge_biketrack, edge_carall], ignore_index = True).drop_duplicates(subset = [\"u\",\"v\",\"osmid\"], ignore_index = True, keep = \"first\").reset_index()\n",
    "G_biketrackcarall = osm_to_ig(node_biketrackcarall, edge_biketrackcarall)\n",
    "\n",
    "round_coordinates(G_biketrack)\n",
    "round_coordinates(G_carall)\n",
    "round_coordinates(G_biketrackcarall)\n",
    "\n",
    "# This loop is only for plotting, executed once, to mirror all y values\n",
    "if not ymirrored:\n",
    "    mirror_y(G_biketrack)\n",
    "    mirror_y(G_carall)\n",
    "    mirror_y(G_biketrackcarall)\n",
    "    ymirrored = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52836 58626\n",
      "12064 11923\n",
      "60748 68604\n",
      "48684 56681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(G_carall.vs), len(G_carall.es))\n",
    "print(len(G_biketrack.vs), len(G_biketrack.es))\n",
    "print(len(G_biketrackcarall.vs), len(G_biketrackcarall.es))\n",
    "print(len(G_biketrackcarall.vs)-len(G_biketrack.vs), len(G_biketrackcarall.es)-len(G_biketrack.es))\n",
    "node_biketrackcarall[\"osmid\"].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_biketrackcarall.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot_reset(G_biketrack, [])\n",
    "ig.plot(G_biketrack, outpath + placeid + '_biketrack.pdf')\n",
    "ig.plot(G_biketrack, outpath + placeid + '_biketrack.png', bbox=(800, 800))\n",
    "ig.plot(G_biketrack, bbox=(400, 300), keep_aspect_ratio = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_plot_reset(G_carall, [])\n",
    "ig.plot(G_carall, outpath + placeid + '_carall.pdf')\n",
    "ig.plot(G_carall, outpath + placeid + '_carall.png', bbox=(800, 800))\n",
    "ig.plot(G_carall, bbox=(400, 300), keep_aspect_ratio = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot_reset(G_biketrackcarall, [])\n",
    "ig.plot(G_biketrackcarall, outpath + placeid + '_biketrackcarall.pdf')\n",
    "ig.plot(G_biketrackcarall, outpath + placeid + '_biketrackcarall.png', bbox=(800, 800))\n",
    "ig.plot(G_biketrackcarall, bbox=(400, 300), keep_aspect_ratio = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_closeness_node(G):\n",
    "    closeness_values = G.closeness(weights = 'weight')\n",
    "    sorted_closeness = sorted(closeness_values, reverse = True)\n",
    "    index = closeness_values.index(sorted_closeness[0])\n",
    "    return G.vs(index)['id']\n",
    "\n",
    "def clusterindices_by_length(clusterinfo, rev = True):\n",
    "    return [k for k, v in sorted(clusterinfo.items(), key=lambda item: item[1][\"length\"], reverse = rev)]\n",
    "\n",
    "class Point:\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "def ccw(A,B,C):\n",
    "    return (C.y-A.y) * (B.x-A.x) > (B.y-A.y) * (C.x-A.x)\n",
    "\n",
    "def segments_intersect(A,B,C,D):\n",
    "    \"\"\"Check if two line segments intersect (except for colinearity)\n",
    "    Returns true if line segments AB and CD intersect properly.\n",
    "    Adapted from: https://stackoverflow.com/questions/3838329/how-can-i-check-if-two-segments-intersect\n",
    "    \"\"\"\n",
    "    if (A.x == C.x and A.y == C.y) or (A.x == D.x and A.y == D.y) or (B.x == C.x and B.y == C.y) or (B.x == D.x and B.y == D.y): return False # If the segments share an endpoint they do not intersect properly\n",
    "    return ccw(A,C,D) != ccw(B,C,D) and ccw(A,B,C) != ccw(A,B,D)\n",
    "\n",
    "def new_edge_intersects(G, enew):\n",
    "    \"\"\"Given a graph G and a potential new edge enew,\n",
    "    check if enew will intersect any old edge.\n",
    "    \"\"\"\n",
    "    E1 = Point(enew[0], enew[1])\n",
    "    E2 = Point(enew[2], enew[3])\n",
    "    for e in G.es():\n",
    "        O1 = Point(e.source_vertex[\"x\"], e.source_vertex[\"y\"])\n",
    "        O2 = Point(e.target_vertex[\"x\"], e.target_vertex[\"y\"])\n",
    "        if segments_intersect(E1, E2, O1, O2):\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def delete_overlaps(G_res, G_orig):\n",
    "    \"\"\"Deletes all overlaps of G_res with G_orig (from G_res)\n",
    "    based on node ids.\n",
    "    \"\"\"\n",
    "    cnt_e = 0\n",
    "    for e in list(G_res.es):\n",
    "        try:\n",
    "            n1_id = e.source_vertex[\"id\"]\n",
    "            n2_id = e.target_vertex[\"id\"]\n",
    "            # If there is already an edge in the original network, delete it\n",
    "            n1_index = G_orig.vs.find(id = n1_id).index\n",
    "            n2_index = G_orig.vs.find(id = n2_id).index\n",
    "            if G_orig.are_connected(n1_index, n2_index):\n",
    "                G_res.delete_edges(e)\n",
    "                cnt_e += 1\n",
    "        except:\n",
    "            pass\n",
    "    # Remove isolated nodes\n",
    "    isolated_nodes = G_res.vs.select(_degree_eq=0)\n",
    "    G_res.delete_vertices(isolated_nodes)\n",
    "    print(\"Removed \" + str(cnt_e) + \" overlapping edges and \" + str(len(isolated_nodes)) + \" nodes.\")\n",
    "\n",
    "\n",
    "def greedy_triangulation(GT, poipairs, betweenness_quantile = 1):\n",
    "    \"\"\"Greedy Triangulation (GT) of a graph GT with an empty edge set.\n",
    "    Distances between pairs of nodes are given by poipairs.\n",
    "    \n",
    "    The GT connects pairs of nodes in ascending order of their distance provided\n",
    "    that no edge crossing is introduced. It leads to a maximal connected planar\n",
    "    graph, while minimizing the total length of edges considered. \n",
    "    See: cardillo2006spp\n",
    "    \"\"\"\n",
    "    \n",
    "    for poipair, poipair_distance in poipairs:\n",
    "        poipair_ind = (GT.vs.find(id = poipair[0]).index, GT.vs.find(id = poipair[1]).index)\n",
    "        if not new_edge_intersects(GT, (GT.vs[poipair_ind[0]][\"x\"], GT.vs[poipair_ind[0]][\"y\"], GT.vs[poipair_ind[1]][\"x\"], GT.vs[poipair_ind[1]][\"y\"])):\n",
    "            GT.add_edge(poipair_ind[0], poipair_ind[1], weight = poipair_distance)\n",
    "            \n",
    "    # Get betweenness for prioritization\n",
    "    BW = GT.edge_betweenness(False, None, \"weight\")\n",
    "    qt = np.quantile(BW, 1-betweenness_quantile)\n",
    "    sub_edges = []\n",
    "    for c, e in enumerate(GT.es):\n",
    "        if BW[c] >= qt: \n",
    "            sub_edges.append(c)\n",
    "        GT.es[c][\"bw\"] = BW[c]\n",
    "        GT.es[c][\"width\"] = math.sqrt(BW[c]+1)*0.5\n",
    "    # Prune\n",
    "    GT = GT.subgraph_edges(sub_edges)\n",
    "    \n",
    "    return GT\n",
    "    \n",
    "\n",
    "def greedy_triangulation_routing_clusters(G, G_total, clusters, clusterinfo, betweenness_quantiles = [1], verbose = False, full_run = False):\n",
    "    \"\"\"Greedy Triangulation (GT) of a bike network G's clusters,\n",
    "    then routing on the graph G_total that includes car infra to connect the GT.\n",
    "    G and G_total are ipgraph graphs\n",
    "    \n",
    "    The GT connects pairs of clusters in ascending order of their distance provided\n",
    "    that no edge crossing is introduced. It leads to a maximal connected planar\n",
    "    graph, while minimizing the total length of edges considered. \n",
    "    See: cardillo2006spp\n",
    "    \n",
    "    Distance here is routing distance, while edge crossing is checked on an abstract \n",
    "    level.\n",
    "    \"\"\"\n",
    "    \n",
    "    # GT is the Graph with the routing of the greedy triangulation\n",
    "    GT_indices = set()\n",
    "    \n",
    "    centroid_indices = [v[\"centroid_index\"] for k, v in sorted(clusterinfo.items(), key=lambda item: item[1][\"size\"], reverse = True)]\n",
    "    G_temp = copy.deepcopy(G_total)\n",
    "    for e in G_temp.es: # delete all edges\n",
    "        G_temp.es.delete(e)\n",
    "    \n",
    "    clusterpairs = clusterpairs_by_distance(G, G_total, clusters, clusterinfo, True, verbose, full_run)\n",
    "    \n",
    "    centroidpairs = [((clusterinfo[c[0][0]]['centroid_id'], clusterinfo[c[0][1]]['centroid_id']), c[2]) for c in clusterpairs]\n",
    "    \n",
    "    GT_abstracts = []\n",
    "    GTs = []\n",
    "    for betweenness_quantile in betweenness_quantiles:\n",
    "        GT_abstract = copy.deepcopy(G_temp.subgraph(centroid_indices))\n",
    "        GT_abstract = greedy_triangulation(GT_abstract, centroidpairs, betweenness_quantile)\n",
    "        GT_abstracts.append(GT_abstract)\n",
    "\n",
    "        centroidids_closestnodeids = {} # dict for retrieveing quickly closest node ids pairs from centroidid pairs\n",
    "        for x in clusterpairs:\n",
    "            centroidids_closestnodeids[(clusterinfo[x[0][0]][\"centroid_id\"], clusterinfo[x[0][1]][\"centroid_id\"])] = (x[1][0], x[1][1])\n",
    "            centroidids_closestnodeids[(clusterinfo[x[0][1]][\"centroid_id\"], clusterinfo[x[0][0]][\"centroid_id\"])] = (x[1][1], x[1][0]) # also add switched version as we do not care about order\n",
    "\n",
    "        # Get node pairs we need to route, sorted by distance\n",
    "        routenodepairs = []\n",
    "        for e in GT_abstract.es:\n",
    "            # get the centroid-ids from closestnode-ids\n",
    "            routenodepairs.append([centroidids_closestnodeids[(e.source_vertex[\"id\"], e.target_vertex[\"id\"])], e[\"weight\"]])\n",
    "\n",
    "        routenodepairs.sort(key=lambda x: x[1])\n",
    "\n",
    "        # Do the routing, on G_total\n",
    "        for poipair, poipair_distance in routenodepairs:\n",
    "            poipair_ind = (G_total.vs.find(id = poipair[0]).index, G_total.vs.find(id = poipair[1]).index)\n",
    "            sp = set(G_total.get_shortest_paths(poipair_ind[0], poipair_ind[1], weights = \"weight\", output = \"vpath\")[0])\n",
    "            GT_indices = GT_indices.union(sp)\n",
    "\n",
    "        GT = G_total.induced_subgraph(GT_indices)\n",
    "        GTs.append(GT)\n",
    "    \n",
    "    return(GTs, GT_abstracts)\n",
    "\n",
    "\n",
    "def clusterpairs_by_distance(G, G_total, clusters, clusterinfo, return_distances = False, verbose = False, full_run = False):\n",
    "    \"\"\"Calculates the (weighted) graph distances on G for a number of clusters.\n",
    "    Returns all pairs of cluster ids and closest nodes in ascending order of their distance. \n",
    "    If return_distances, then distances are also returned.\n",
    "\n",
    "    Returns a list containing these elements, sorted by distance:\n",
    "    [(clusterid1, clusterid2), (closestnodeid1, closestnodeid2), distance]\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_indices = clusterindices_by_length(clusterinfo, False) # Start with the smallest so the for loop is as short as possible\n",
    "    clusterpairs = []\n",
    "    clustercopies = {}\n",
    "    \n",
    "    # Create copies of all clusters\n",
    "    for i in range(len(cluster_indices)):\n",
    "        clustercopies[i] = clusters[i].copy()\n",
    "        \n",
    "    # Take one cluster\n",
    "    for i, c1 in enumerate(cluster_indices[:-1]):\n",
    "        c1_indices = G_total.vs.select(lambda x: x[\"id\"] in clustercopies[c1].vs()[\"id\"]).indices\n",
    "        print(\"Working on cluster \" + str(i+1) + \" of \" + str(len(cluster_indices)) + \"...\")\n",
    "        for j, c2 in enumerate(cluster_indices[i+1:]):\n",
    "            closest_pair = {'i': -1, 'j': -1}\n",
    "            min_dist = np.inf\n",
    "            c2_indices = G_total.vs.select(lambda x: x[\"id\"] in clustercopies[c2].vs()[\"id\"]).indices\n",
    "            if verbose: print(\"... routing \" + str(len(c1_indices)) + \" nodes to \" + str(len(c2_indices)) + \" nodes in other cluster \" + str(j+1) + \" of \" + str(len(cluster_indices[i+1:])) + \".\")\n",
    "            \n",
    "            if full_run:\n",
    "                # Compare all pairs of nodes in both clusters (takes long)\n",
    "                for a in list(c1_indices):\n",
    "                    sp = G_total.get_shortest_paths(a, c2_indices, weights = \"weight\", output = \"epath\")\n",
    "\n",
    "                    if all([not elem for elem in sp]):\n",
    "                        # If there is no path from one node, there is no path from any node\n",
    "                        break\n",
    "                    else:\n",
    "                        for path, c2_index in zip(sp, c2_indices):\n",
    "                            if len(path) >= 1:\n",
    "                                dist_nodes = sum([G_total.es[e]['weight'] for e in path])\n",
    "                                if dist_nodes < min_dist:\n",
    "                                    closest_pair['i'] = G_total.vs[a][\"id\"]\n",
    "                                    closest_pair['j'] = G_total.vs[c2_index][\"id\"]\n",
    "                                    min_dist = dist_nodes\n",
    "            else:\n",
    "                # Do a heuristic that should be close enough.\n",
    "                # From cluster 1, look at all shortest paths only from its centroid\n",
    "                a = clusterinfo[c1][\"centroid_index\"]\n",
    "                sp = G_total.get_shortest_paths(a, c2_indices, weights = \"weight\", output = \"epath\")\n",
    "                if all([not elem for elem in sp]):\n",
    "                    # If there is no path from one node, there is no path from any node\n",
    "                    break\n",
    "                else:\n",
    "                    for path, c2_index in zip(sp, c2_indices):\n",
    "                        if len(path) >= 1:\n",
    "                            dist_nodes = sum([G_total.es[e]['weight'] for e in path])\n",
    "                            if dist_nodes < min_dist:\n",
    "                                closest_pair['j'] = G_total.vs[c2_index][\"id\"]\n",
    "                                min_dist = dist_nodes\n",
    "                # Closest c2 node to centroid1 found. Now find all c1 nodes to that closest c2 node.\n",
    "                b = G_total.vs.find(id = closest_pair['j']).index\n",
    "                sp = G_total.get_shortest_paths(b, c1_indices, weights = \"weight\", output = \"epath\")\n",
    "                if all([not elem for elem in sp]):\n",
    "                    # If there is no path from one node, there is no path from any node\n",
    "                    break\n",
    "                else:\n",
    "                    for path, c1_index in zip(sp, c1_indices):\n",
    "                        if len(path) >= 1:\n",
    "                            dist_nodes = sum([G_total.es[e]['weight'] for e in path])\n",
    "                            if dist_nodes <= min_dist: # <=, not <!\n",
    "                                closest_pair['i'] = G_total.vs[c1_index][\"id\"]\n",
    "                                min_dist = dist_nodes\n",
    "            \n",
    "            if closest_pair['i'] != -1 and closest_pair['j'] != -1:\n",
    "                clusterpairs.append([(c1, c2), (closest_pair['i'], closest_pair['j']), min_dist])\n",
    "                                    \n",
    "    clusterpairs.sort(key = lambda x: x[-1])\n",
    "    if return_distances:\n",
    "        return clusterpairs\n",
    "    else:\n",
    "        return [[o[0], o[1]] for o in clusterpairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = copy.deepcopy(G_biketrack) # G is the bike graph we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 largest clusters of 297 considered. Length covered: 304.90 km (80% of total length)\n"
     ]
    }
   ],
   "source": [
    "clusters = []\n",
    "clusterinfo = {}\n",
    "i = 0\n",
    "total_length = sum(G.es[\"weight\"])\n",
    "for j in range(len(list(G.components()))):\n",
    "    if len(list(G.components())[j]) > 1:\n",
    "        clusterinfo[i] = {\"size\": G.subgraph(list(G.components())[j]).vcount(), \n",
    "                          \"centroid_id\": highest_closeness_node(G.subgraph(list(G.components())[j]))[0],\n",
    "                          \"length\": sum(G.subgraph(list(G.components())[j]).es[\"weight\"])\n",
    "                          }\n",
    "        clusterinfo[i][\"centroid_index\"] = G.vs.find(id = clusterinfo[i]['centroid_id']).index\n",
    "        i += 1\n",
    "\n",
    "\n",
    "cluster_indices = clusterindices_by_length(clusterinfo)\n",
    "\n",
    "clusterinfo_temp = {}\n",
    "length_covered = 0\n",
    "i = 0\n",
    "for c in cluster_indices:\n",
    "    clusters.append(G.subgraph(list(G.components())[c]))\n",
    "    clusterinfo_temp[i] = clusterinfo[c]\n",
    "    length_covered += clusterinfo[c][\"length\"]\n",
    "    if length_covered >= cutoff*total_length:\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "print('{:d}'.format(i+1) + \" largest clusters of \" + '{:d}'.format(len(list(G.components()))) + \" considered. Length covered: \" + '{:.2f}'.format(length_covered) + \" km (\" + '{:.0f}'.format(100*length_covered/total_length) + \"% of total length)\")\n",
    "\n",
    "clusterinfo = copy.deepcopy(clusterinfo_temp)\n",
    "\n",
    "cluster_indices = clusterindices_by_length(clusterinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pp.pprint(clusterinfo)\n",
    "\n",
    "plt.figure(figsize=[2*6.4, 2*4.8])\n",
    "for i in range(len(clusters)):\n",
    "    plt.plot(clusters[i].vs['x'],clusters[i].vs['y'],'.')\n",
    "plt.gca().invert_yaxis()\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pp.pprint(clusterinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # For testing\n",
    "# # Get the pairs of distances between all clusters, where distance is routing distance of the closest nodes\n",
    "# clusterpairs = clusterpairs_by_distance(G, G_biketrackcarall, clusters, clusterinfo, True, False, False)\n",
    "# clusterpairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For testing\n",
    "# temp_list = set()\n",
    "# for x in clusterpairs:\n",
    "#     temp_list.add(x[1][0])\n",
    "#     temp_list.add(x[1][1])\n",
    "# temp_list = list(temp_list)\n",
    "# temp_list\n",
    "# my_plot_reset(G_biketrack, temp_list)\n",
    "# ig.plot(G_biketrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GTs, GT_abstracts = greedy_triangulation_routing_clusters(G, G_biketrackcarall, clusters, clusterinfo, BWqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for GT in GTs:\n",
    "    delete_overlaps(GT, G_biketrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ig.plot(GT_abstracts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot just the clusters and their new connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[2*6.4, 2*4.8])\n",
    "plt.plot(GTs[-1].vs['x'], GTs[-1].vs['y'], 'o', color='red', markersize=5)\n",
    "for i in range(len(clusters)):\n",
    "    plt.plot(clusters[i].vs['x'], clusters[i].vs['y'], 'o', markersize=3)\n",
    "plt.gca().invert_yaxis()\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot just the new connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_nodes(GTs[-1], 0)\n",
    "# width_edges(GTs[-1], 2)\n",
    "# color_edges(GTs[-1], \"blue\")\n",
    "\n",
    "# ig.plot(GTs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the whole bike network and its new connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for GT, BWq in zip(GTs, BWqs):\n",
    "    plt.figure(figsize=[2*6.4, 2*4.8])\n",
    "    plt.plot(GT.vs['x'], GT.vs['y'], 'o', color='red', markersize=3)\n",
    "    plt.plot(G_biketrack.vs['x'], G_biketrack.vs['y'], 'o', color='black', markersize=1)\n",
    "    plt.gca().invert_yaxis()\n",
    "    %config InlineBackend.figure_format = 'retina'\n",
    "    plt.savefig(outpath + placeid + '_GTclusters_biketrack_cutoff' + \"{:.2f}\".format(cutoff) + \"_\" + \"BWq{:.2f}\".format(BWq) + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OSMNX",
   "language": "python",
   "name": "osmnx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
