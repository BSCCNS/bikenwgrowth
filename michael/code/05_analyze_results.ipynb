{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Analysis of bicycle network results\n",
    "## Project: Algorithmic bicycle network design\n",
    "#### Michael Szell, Tyler Perlman, Sayat Mimar, Gourab Ghoshal, Roberta Sinatra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the existing infrastructure, the results from 03_poi_based_generation and 04_connect_clusters and calculates/analyzes a number of measures:\n",
    "* cost (length)\n",
    "* coverage  \n",
    "* directness  \n",
    "* efficiency\n",
    "\n",
    "Contact: Michael Szell (michael.szell@gmail.com)  \n",
    "Created: 2020-07-08  \n",
    "Last modified: 2020-07-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False # If True, will produce plots and/or verbose output to double-check\n",
    "%run -i \"../parameters/parameters.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i path.py\n",
    "%run -i setup.py\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze existing infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for placeid, placeinfo in cities.items():\n",
    "    print(placeid + \": Analying existing infrastructure.\")\n",
    "    \n",
    "    # output_place is one static file for the existing city. This can be compared to the generated infrastructure.\n",
    "    # Make a check if this file was already generated - it only needs to be done once. If not, generate it:\n",
    "    filename = placeid + \"_existing.csv\"\n",
    "    if not os.path.isfile(PATH[\"results\"] + placeid + \"/\" + filename):\n",
    "        empty_metrics = {\n",
    "                         \"length\":0,\n",
    "                         \"coverage\": 0,\n",
    "                         \"directness\": 0,\n",
    "                         \"poi_coverage\": 0,\n",
    "                         \"components\": 0,\n",
    "                         \"efficiency_global\": 0,\n",
    "                         \"efficiency_local\": 0\n",
    "                        }\n",
    "        output_place = {}\n",
    "        for networktype in networktypes:\n",
    "            output_place[networktype] = copy.deepcopy(empty_metrics)\n",
    "\n",
    "        # Analyze all networks     \n",
    "        Gs = {}\n",
    "        for networktype in networktypes:\n",
    "            Gs[networktype] = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, networktype)\n",
    "            Gs[networktype + \"_simplified\"] = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, networktype + \"_simplified\")\n",
    "        \n",
    "        with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_railwaystation_nnidscarall.csv') as f:\n",
    "            nnids = [int(line.rstrip()) for line in f]\n",
    "\n",
    "        for networktype in networktypes:\n",
    "            if debug: print(placeid + \": Analying results: \" + networktype)\n",
    "            # To do: Call calculate_metrics for existing infrastructure, passing ox-simplified graphs as GT_abstract\n",
    "            metrics = calculate_metrics(Gs[networktype], Gs[networktype + \"_simplified\"], Gs['carall'], nnids, buffer_walk, numnodepairs, True)\n",
    "            for key, val in metrics.items():\n",
    "                output_place[networktype][key] = val\n",
    "        \n",
    "        # Write to CSV\n",
    "        # https://stackoverflow.com/questions/29400631/python-writing-nested-dictionary-to-csv\n",
    "        with open(PATH[\"results\"] + placeid + \"/\" + filename, 'w') as f:\n",
    "            fields = ['network', 'components', 'coverage', 'directness', 'length', 'poi_coverage', 'efficiency_global', 'efficiency_local']\n",
    "            w = csv.DictWriter(f, fields)\n",
    "            w.writeheader()\n",
    "            for key, val in sorted(output_place.items()):\n",
    "                row = {'network': key}\n",
    "                row.update(val)\n",
    "                w.writerow(row)\n",
    "        # TO DO: TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze POI based results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for placeid, placeinfo in cities.items():\n",
    "    print(placeid + \": Analying results\")\n",
    "\n",
    "    # Load networks\n",
    "    G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "    \n",
    "    # Load POIs\n",
    "    with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + poi_source + '_nnidscarall.csv') as f:\n",
    "        nnids = [int(line.rstrip()) for line in f]\n",
    "            \n",
    "    # Load results\n",
    "    filename = placeid + '_poi_' + poi_source + \"_\" + prune_measure\n",
    "    resultfile = open(PATH[\"results\"] + placeid + \"/\" + filename + \".pickle\",'rb')\n",
    "    res = pickle.load(resultfile)\n",
    "    resultfile.close()\n",
    "    if debug: pp.pprint(res)\n",
    "        \n",
    "        \n",
    "    # output contains lists for all the prune_quantile values of the corresponding results\n",
    "    output = {\"length\":[],\n",
    "              \"coverage\": [],\n",
    "              \"directness\": [],\n",
    "              \"poi_coverage\": [],\n",
    "              \"components\": [],\n",
    "              \"efficiency_global\": [],\n",
    "              \"efficiency_local\": []\n",
    "             }\n",
    "\n",
    "    for GT, GT_abstract, prune_quantile in zip(res[\"GTs\"], res[\"GT_abstracts\"], res[\"prune_quantiles\"]):\n",
    "        if debug: print(prune_quantile, len(GT.vs))\n",
    "\n",
    "        metrics = calculate_metrics(GT, GT_abstract, G_carall, nnids, buffer_walk, numnodepairs)\n",
    "\n",
    "        for key in output.keys():\n",
    "            output[key].append(metrics[key])\n",
    "        \n",
    "    # Write to CSV\n",
    "    filename = placeid + '_poi_' + poi_source + \"_\" + prune_measure + \".csv\"\n",
    "\n",
    "    with open(PATH[\"results\"] + placeid + \"/\" + filename, 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(output.keys())\n",
    "        w.writerows(zip(*output.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    pp.pprint(output)\n",
    "    \n",
    "    fig = plt.figure(figsize=(4, 3)) # create figure object with a (width,height)\n",
    "    axes = fig.add_axes([0, 0, 1, 1]) # left, bottom, width, height (range 0 to 1)\n",
    "    for pol in cov: # cov is generally a MultiPolygon, so we iterate through its Polygons\n",
    "        y, x = pol.exterior.coords.xy\n",
    "        patch1 = matplotlib.patches.Polygon(np.stack((np.asarray(x), -1*np.asarray(y)), axis=-1))\n",
    "        axes.add_patch(patch1)\n",
    "    axes.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze cluster based results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "%run -i functions.py\n",
    "for placeid, placeinfo in cities.items():\n",
    "    print(placeid + \": Analyzing results\")\n",
    "\n",
    "    # Load networks\n",
    "    G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "    \n",
    "    # Load POIs\n",
    "    with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + poi_source + '_nnidscarall.csv') as f:\n",
    "        nnids = [int(line.rstrip()) for line in f]\n",
    "    \n",
    "    # Load results\n",
    "    filename = placeid + '_clusters_' + prune_measure + \"_cutoff\" + cutofftype + \"{:.2f}\".format(cutoff) + \".pickle\"\n",
    "    resultfile = open(PATH[\"results\"] + placeid + \"/\" + filename,'rb')\n",
    "    res = pickle.load(resultfile)\n",
    "    resultfile.close()\n",
    "    if debug: pp.pprint(res)\n",
    "        \n",
    "        \n",
    "    # output contains lists for all the prune_quantile values of the corresponding results\n",
    "    output = {\"length\":[],\n",
    "              \"coverage\": [],\n",
    "              \"directness\": [],\n",
    "              \"poi_coverage\": [],\n",
    "              \"components\": [],\n",
    "              \"efficiency_global\": [],\n",
    "              \"efficiency_local\": []\n",
    "             }\n",
    "\n",
    "    for GT, GT_abstract, prune_quantile in zip(res[\"GTs\"], res[\"GT_abstracts\"], res[\"prune_quantiles\"]):\n",
    "        if debug: print(prune_quantile, len(GT.vs))\n",
    "\n",
    "        metrics = calculate_metrics(GT, GT_abstract, G_carall, nnids, buffer_walk, numnodepairs)\n",
    "\n",
    "        for key in output.keys():\n",
    "            output[key].append(metrics[key])\n",
    "        \n",
    "    # Write to CSV\n",
    "    filename = placeid + '_clusters_' + prune_measure + \"_cutoff\" + cutofftype + \"{:.2f}\".format(cutoff) + \".csv\"\n",
    "    with open(PATH[\"results\"] + placeid + \"/\" + filename, 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(output.keys())\n",
    "        w.writerows(zip(*output.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(sound_file, autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OSMNX",
   "language": "python",
   "name": "osmnx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
